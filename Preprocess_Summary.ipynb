{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#integrating preprocessing functions together\n",
    "def preprocess(text):\n",
    "    # import libraries\n",
    "    import spacy\n",
    "    from spacy.lang.en import English\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.tag import pos_tag\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    import re\n",
    "    \n",
    "    df1=pd.DataFrame()\n",
    "    df1['senten']=[text]\n",
    "    \n",
    "    #cleaning text\n",
    "    def clean(text):\n",
    "        text = re.sub('[0-9]+.\\t','',str(text))\n",
    "        text = re.sub('\\n ','',str(text))\n",
    "        text = re.sub('\\n',' ',str(text))\n",
    "        text = re.sub(\"'s\",'',str(text))\n",
    "        text = re.sub(\"-\",' ',str(text))\n",
    "        text = re.sub(\"â€” \",'',str(text))\n",
    "        text = re.sub('\\\"','',str(text))\n",
    "        text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
    "        text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
    "        text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "        return text\n",
    "\n",
    "    df1['sent']=df1['senten'].apply(clean)\n",
    "    #splitting into sentence\n",
    "    def sentences(text):\n",
    "        # split sentences and questions\n",
    "        i=0\n",
    "        text = re.split('[.?]', text)\n",
    "        clean_sent = []\n",
    "        for sent in text:\n",
    "            clean_sent.append(sent)\n",
    "            df1.loc[i,'senten'] = (sent)\n",
    "            i+=1\n",
    "        return clean_sent\n",
    "\n",
    "    df1['sent']=df1['sent'].apply(sentences)\n",
    "    df2=pd.DataFrame(columns={'senten','token','lemma','pairs','relations'})\n",
    "    df2['senten']=df1['senten']\n",
    "    \n",
    "    # word tokenizer\n",
    "    i=0\n",
    "    for sent in df2['senten']:\n",
    "        token_list=[]\n",
    "        tokens=word_tokenize(df2.loc[i,'senten'])\n",
    "        for token in tokens:\n",
    "            token_list.append(token)\n",
    "        df2.loc[i,'token']=token_list\n",
    "        i+=1\n",
    "    # lemmatizing\n",
    "    all_lemma=[]\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "    i=0\n",
    "    for i in range(len(df2['token'])):\n",
    "        lemma_list=[]\n",
    "        tokens=[]\n",
    "        tokens=tokenizer(df2.loc[i,'senten'])\n",
    "        for w in tokens:\n",
    "            lemma_list.append(w.lemma_)\n",
    "            all_lemma.append(w.lemma_)\n",
    "        df2.loc[i,'lemma']=lemma_list\n",
    "        i+=i  \n",
    "        \n",
    "        \n",
    "    #joining back tokens to sentence\n",
    "    i=0\n",
    "    for i in range(len(df2['token'])):\n",
    "        df2.loc[i,'senten']=' '.join(df2.loc[i,'lemma'])\n",
    "        i+=1\n",
    "       \n",
    "   #subject_object pairs\n",
    "    import spacy\n",
    "    from spacy import displacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def get_entities(sent):\n",
    "        ent1 = \"\"\n",
    "        ent2 = \"\"\n",
    "        prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "        prv_tok_text = \"\"   # previous token in the sentence\n",
    "    \n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "  \n",
    "        for tok in nlp(sent):\n",
    "         ## chunk 2\n",
    "         # if token is a punctuation mark then move on to the next token\n",
    "            if tok.dep_ != \"punct\":\n",
    "                # check: token is a compound word or not\n",
    "                if tok.dep_ == \"compound\":\n",
    "                    prefix = tok.text\n",
    "       \n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "                    if prv_tok_dep == \"compound\":\n",
    "                        prefix = prv_tok_text + \" \"+ tok.text\n",
    "                    \n",
    "            # check: token is a modifier or not      \n",
    "                if tok.dep_.endswith(\"mod\") == True:\n",
    "                    modifier = tok.text\n",
    "            \n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "                    if prv_tok_dep == \"compound\":\n",
    "                        modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3  captures subjects \n",
    "                if tok.dep_.find(\"subj\") == True:\n",
    "                    ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                    prefix = \"\"\n",
    "                    modifier = \"\"\n",
    "                    prv_tok_dep = \"\"\n",
    "                    prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4  captures objects\n",
    "                if tok.dep_.find(\"obj\") == True:\n",
    "                     ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      # update variables\n",
    "                prv_tok_dep = tok.dep_\n",
    "                prv_tok_text = tok.text\n",
    "\n",
    "        return [ent1.strip(), ent2.strip()]\n",
    "    #call for entity extraction\n",
    "    def entity_extract():\n",
    "        i=0\n",
    "        for i in range(len(df2['senten'])):\n",
    "            entity_pairs=[]\n",
    "            entity_pairs.append(get_entities(df2.loc[i,'senten']))\n",
    "            df2.loc[i,'pairs']=entity_pairs\n",
    "            i+=1\n",
    "    \n",
    "    entity_extract()\n",
    "    #spacy rule based matching\n",
    "    from spacy.matcher import Matcher \n",
    "    from spacy.tokens import Span \n",
    "\n",
    "    def get_relation(sent):\n",
    "        doc = nlp(sent)\n",
    "      # Matcher class object \n",
    "        matcher = Matcher(nlp.vocab)\n",
    "\n",
    "      #define the pattern \n",
    "        pattern = [{'DEP':'ROOT'}, \n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},  \n",
    "                {'POS':'ADJ','OP':\"?\"}] \n",
    "        matcher.add(\"Matching\", None, pattern) \n",
    "        matches = matcher(doc)     #returns match_id, start and stop indexes of the matched words\n",
    "        span = doc[matches[0][1]:matches[0][2]] \n",
    "        return(span.text)\n",
    "    #call for relation extraction\n",
    "    def relation_extract():\n",
    "        relations=[]\n",
    "        i=0\n",
    "        for i in range(len(df2['token'])):\n",
    "            relations=[]\n",
    "            relations=get_relation(df2.loc[i,'senten'])\n",
    "            df2.loc[i,'relations']=(relations)\n",
    "            i+=1\n",
    "    \n",
    "    relation_extract()\n",
    "    #building knowledge graph\n",
    "    # extract subject\n",
    "    i=0\n",
    "    source=[]\n",
    "    target=[]\n",
    "    src=[]\n",
    "    relation=[]\n",
    "    for i in range(50):\n",
    "        src=df2.loc[i,'pairs']\n",
    "        source.append(src[0][0])\n",
    "        target.append(src[0][1])\n",
    "        relation.append(df2.loc[i,'relations'])\n",
    "        i+=1\n",
    "\n",
    "    kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relation})\n",
    "    #plotting graph\n",
    "    # create a directed-graph from a dataframe\n",
    "    G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\",  edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "    plt.figure(figsize=(8,8))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "    plt.savefig('graph_new.png')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    return df2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary\n",
    "def summary(scraped_data):\n",
    "    import bs4 as bs\n",
    "    import urllib.request\n",
    "    import re\n",
    "    import nltk\n",
    "    #scraped_data=open(\"./UNGDC+1970-2018/Converted sessions/Session 25 - 1970/IND_25_1970.txt\")\n",
    "    article=scraped_data.read()\n",
    "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "    article_text = \"\"\n",
    "    for p in paragraphs:\n",
    "        article_text += p.text\n",
    "    article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "    article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "    sentence_list = nltk.sent_tokenize(article_text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_article_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "                \n",
    "    # weighted freq\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy) \n",
    "    \n",
    "    # Calculating Sentence Scores\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "    import heapq\n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    summary = ' '.join(summary_sentences)    \n",
    "                \n",
    "    return summary   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
